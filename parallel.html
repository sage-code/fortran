<!DOCTYPE html>
<html lang="en">
<head>
       
<meta charset="utf-8">
<meta name="description" content="Fortran parallel computing and high performance computing.">
<meta name="author" content="Elucian Moise">
<meta name="keywords" content="sage, fortran, parallel, computing">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- Website title -->
<title>Parallel Computing</title>  

<!-- Bootstrap CSS -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<!-- Icon -->
<link rel="icon" type="image/png"  href="../images/favicon.ico">  
<!-- Prism Highlighter -->
<link rel="stylesheet" href="../prism.css">
<script src="../prism.js"></script>
<!-- custom css -->
<link rel="stylesheet" href="../sage.css">
</head>

<body>

<div class="container">
        
<!-- header -->
<div class="row">
    <div class="col">
        <a href="https://sagecode.net"><img src="../images/sage-logo.svg" alt="Sage-Code Laboratory" height="80"></a>
    </div>  
    <div class="col  bottom-right">
        <a href="index.html#lang-index">index</a>&lt;--
    </div>
</div><hr>


<h1>Parallel Computing</h1>


<div class="alert alert-secondary shadow-sm">In Fortran there are several programming models you can use to implement a parallel algorithm. Most models are using special compiler options and additional libraries. Native compilation does not use parallel computing by default. Our investigation is work in progress!</div>

<h2>Efficiency vs Performance</h2>

<p>Before making your application run in parallel, you should consider what your goal is. You must see the larger picture to avoid hard work with little gain. Using parallelization will improve performance but will always reduce overall efficiency of the system.</p>

<p>Performance is measured in time gained. This is good in general but sometimes it may be too expensive. If the server is already busy running many processes, starting a Fortran program that is consuming all the resources will slow down other applications that are already running in the background and are overheating the system, reducing performance.</p>

<h3>Execution Overhead</h3>

<p>Parallel processing is not always a good idea. A parallel program is wasting time, splitting a job in smaller parts to be run in parallel, then aggregating the partial results. This overhead can increase the total processing time. After putting effort in coding a faster program, you end up with a program that is slower than the single thread version.</p>

<h3>I/O optimization</h3>

<p>Most blocking operations in a global process are I/O operations on diverse devices. Mechanical hard-disks & optical-disks are slow. Lately there are SSD storage devices that are much faster. Using better hardware can improve I/O performance significantly.</p>

<p>A good idea is to read data from one source and write data in parallel on different files. Having one file output/process will separate the processes and will improve performance. The problem is, after you finish processing you must aggregate the partial results back into a single file using a single thread.</p>

<h3>Compiler optimization</h3>

<p>Fortran compilers can target a specific platform. You can use compiler flags (options) to optimize the generated code and take advantage of special microprocessor features, specific to a platform that can improve performance of your application significantly. That may be a good alternative before using parallelization.</p>

<h3>System parallelization</h3>

<p>You can use Bash to run processes in parallel using the operating system multitasking capability. You can create a Bash script that starts different processes in the background. This is the most easy way to create a parallel process.</p>

<!-- a regular diagram -->
<div class="text-center">
  <img src="img/map_reduce.svg" alt="Map Reduce Diagram"
       width="600" class="img-fluid protect rounded shadow border" >
  <p>Map Reduce Model</p>
</div>

<h4>Advantages</h4>

<p>Using this model, parallel processes can have different complexity levels and different duration. For large projects, you can organize processes using a job scheduler. You learn this technology in a Software Engineering course.</p>

<h4>Disadvantages</h4>

<p>Interprocess communication is difficult. Also you have to use two programming languages. Data transfer is expensive and there is a lot of code that you must create to receive data from input, parse the data and create the output data.</p>

<h3>Loop parallelization</h3>

<p>Loops are computing intensive and can be run in parallel. You can use the Fortran compiler to parallelize loops using multi-threading. After the loop is finalized the main thread can aggregate the result. Not all loops can be executed in parallel. If a process is using nested loops, probably only the outer loop will be executed in parallel.</p>

<h3>Competition for resources</h3>

<p>If you are not careful, running <em> system processes  </em> simultaneously with <em> loop parallelization  </em> will cause a competition for resources. This can overwhelm the server, and slow down all the processes. Maybe a better idea is to use multiple computers connected in a network if you need more computing power. This technique of parallelization is called <em>Distributed Computing</em>.</p>

<h3>Automatic parallelization</h3>

<p>Some older versions of compilers may have automatic parallelization features. The idea was: you do not need to modify your program. The compiler can decide if your program can be optimized using parallel computing. This model of parallelization seams to be abandoned and replaced by explicit parallelization models.</p>

<h3>Explicit parallelization</h3>

<p>For better control over parallelization you need to use <em> compiler directives</em> that trigger parallel code generators and enable specific parts of the application to run in parallel. You need skills to read, create and debug code designed for parallel execution.</p>

<h2>Parallelization methods</h2>

<p>Fortran has different methods of parallelization that can be used to increase process performance. Different compilers are implementing the parallelization standards in different ways. Fortran specification is hazy and unclear (on purpose). Here is a list of methods we have identified:</p>

<ol>
<li><a href="#concurent">do concurrent (loops)</a></li> 
<li><a href="#openMP">openMP (Open Multi Processing)</a></li>   
<li><a href="#coarrays">coarrays (CAF) Co-Array Fortran</a></li>
<li><a href="#MPI">MPI (Message Passing Interface)</a></li>
</ol>

<div class="alert alert-warning"><b>Disclaim: </b> Next code snippets are not runnable. These are fragments of code. You must research specific methods and compiler flags to generate parallel code. Check the compiler reference books before designing your code for a specific platform.</div>

<h3><a id="concurent">do concurrent (loops)</a></h3> 

<p>Fortran 2008 specification describes a new kind of loop. Do loop is augmented with keyword "CONCURRENT" or "concurrent". This enables more effective parallel execution of native Fortran code without the use of non-standard directives.</p>

<p>Declaring the loop concurrently enables the compiler to decide if the loop is good enough to be executed in parallel. The intention to run in parallel requires you to follow several restrictions, otherwise the compiler will not enable parallel execution.</p>

<h4>Restrictions</h4>

<ol>
<li>Do not use interruption statements that would prevent the loop from executing all its iterations: RETURN, EXIT, GOTO, CYCLE.</li>
<li>Do not use image control statements: STOP, SYNC, LOCK/UNLOCK, EVENT;
<li>Do not ALLOCATE/DEALLOCATE coarrays inside the loop or nested loop or any other subprogram called from the loop.</li>
<li>Calling a procedure that is not PURE from inside the loop. A pure procedure do not have side effects.</li>
<li>Deallocation of any polymorphic entity, as that could cause an impure FINAL subroutine to be called.</li>
<li>You can't mess with the IEEE floating-point control and status flags.</li>
<li>You can't modify an object in one iteration and expect to be able to read it in another.</li>
</ol>

<h4>Advantages</h4>

<p>In return for accepting these restrictions, a DO CONCURRENT might compile into code that exploits the parallel features of the target machine to run the iterations of the DO CONCURRENT construct without using any OpenACC or OpenMP directive.</p>

<!-- code block -->
<pre><code class="language-fortran line-numbers">! fortran fragment
integer,dimension(n) :: j, k
integer :: i, m
m = 10
i = 15
do concurrent (i = 1:n, j(i)> 0) local (m) shared (j, k)
   m =  mod (k(i), j(i))
   k(i) = k(i) â€“ m
end do
print *, i, m] ! expected 15 10
</code></pre>

<h4>External References</h4>

<ul>
    <li><a href="https://flang.llvm.org/docs/DoConcurrent.html" target="_blank">LLVM Article</a></li>
    <li><a href="https://www.intel.com/content/www/us/en/develop/documentation/fortran-compiler-oneapi-dev-guide-and-reference/top/language-reference/a-to-z-reference/c-to-d/do-concurrent.html" target="_blank">Intel Reference</a></li>
    <li><a href="https://developer.nvidia.com/blog/accelerating-fortran-do-concurrent-with-gpus-and-the-nvidia-hpc-sdk/" target="_blank">NVIDIA Concurency</a></li>
</ul>

<!-- work in progress-->
<h3><a id="openMP">openMP</a> (Open Multi Processing)</h3>

<p>OpenMP (Open Multi-Processing) is an application programming interface (API) that supports multi-platform shared-memory. It consists of a set of compiler directives, library routines, and environment variables that enable run-time parallelization.</p>

<!-- code block -->
<pre><code class="language-fortran line-numbers">
!$OMP PARALLEL DO DEFAULT(NONE) PRIVATE(i) REDUCTION(+:pi)
do i = 1, limit
  pi = pi + (-1)**(i+1) / real( 2*i-1, kind=rk )
end do
!$OMP END PARALLEL DO
</code></pre>

<h4>External References</h4>

<a href="https://en.wikipedia.org/wiki/OpenMP" target="_blank"
>Wikipedia Open MP</a>


<!-- work in progress-->
<h3><a id="coarrays">coarrays</a> (CAF) Co-Array Fortran</h3>

<p>Fortran 2008 contains the coarray parallel. It is the first time that a parallel programming model has been added to the language as a standard feature, portable across all platforms. Compilers supporting the model are available or under development from all the major compiler vendors.</p>

<p>The coarray programming model consists of two new features added to the language, an extension of the normal array syntax to represent data decomposition plus an extension to the execution model to control parallel work distribution.</p>

<h4>Execution Model</h4>

<p>The coarray execution model is based on the Single Program Multiple Data (SPMD). A CAF program is replicated a number of times. Each copy has its own set of data objects and is named <em>image</em>. All images are executed asynchronously.</p>

<!-- code block -->
<pre><code class="language-fortran line-numbers">!coarray declaration
real :: a(n)[*]
complex :: z[0:*]
integer :: index(n)[*]
real :: b(n)[p, *]
real :: c(n,m)[0:p, -3:q, +3:*]
real, allocatable :: w(:)[:,:]
type(field),allocatable :: max[:,:]
</code></pre>

<h4>External References</h4>

<a href="https://en.wikipedia.org/wiki/Coarray_Fortran" target="_blank"
>Wikipedia Coarrays</a>


<!-- work in progress-->
<h3><a id="MPI">MPI</a> (Message Passing Interface)</h3>

<p>Message Passing Interface (MPI) is a communication protocol for parallel programming. MPI is specifically used to allow applications to run in parallel across a number of separate computers connected by a network.</p>

<h4>Distributed system</h4>

<p>A distributed system consists of a collection of autonomous computers, connected through a network and distribution middleware, which enables computers to coordinate their activities and to share the resources of the system so that users perceive the system as a single, integrated computing facility.</p>

<h4>Advantages</h4>

<ul>
    <li>Highly efficient</li>
    <li>Scalability</li>
    <li>Less tolerant of failures</li>
    <li>High Availability</li>
</ul>

<!-- code block -->
<pre><code class="language-fortran line-numbers"
>program mpi
   include 'mpif.h'
   integer rank, size, ierror, tag, status(MPI_STATUS_SIZE)
   
   call MPI_INIT(ierror)
   call MPI_COMM_SIZE(MPI_COMM_WORLD, size, ierror)
   call MPI_COMM_RANK(MPI_COMM_WORLD, rank, ierror)
   print*, 'node', rank, ': Hello world'
   call MPI_FINALIZE(ierror)
end program
</code></pre>

<h4>References</h4>

<ul>
<li><a href="https://www.open-mpi.org/" target="_blank">
    Open MPI Homepage</a></li>
<li><a href="https://hpc-wiki.info/hpc/How_to_Use_MPI" target="_blank">
    How to use MPI</a></li>
</ul>

<h2>Open MP vs MPI</h2>

<p>Until Fortran is establishing the standard we can use one of these: Open MP or MPI. Selecting the right one is an Engineering decision. Here are some considerations:</p>

<hr>
<h4>Pros of OpenMP</h4>
<ul>
<li>easier to program and debug than MPI</li>
<li>directives can be added incrementally - gradual parallelization</li>
<li>can still run the program as a serial code</li>
<li>serial code statements usually don't need modification</li>
<li>code is easier to understand and maybe more easily maintained</li>
</ul>
<h4>Cons of OpenMP</h4>
<ul>
<li>can only be run in shared memory computers</li>
<li>requires a compiler that supports OpenMP</li>
<li>mostly used for loop parallelization</li>
</ul>
<hr>
<h4>Pros of MPI</h4>
<ul>
<li>runs on either shared or distributed memory architectures</li>
<li>can be used on a wider range of problems than OpenMP</li>
<li>each process has its own local variables</li>
<li>distributed memory computers are less expensive than large shared memory computers</li>
</ul>
<h4>Cons of MPI</h4>
<ul>    
<li>requires more programming changes to go from serial to parallel version</li>
<li>can be harder to debug</li>
<li>performance is limited by the communication network between the nodes</li>
</ul>

<!-- work in progress-->
<hr>
<p><b>Go back:</b>
<a href="index.html">Fortran Tutorial</a></p>

<!-- Footer -->
<footer class="footer">
  <div class="footer-copyright text-center"></div>
</footer>

</div></body></html>

